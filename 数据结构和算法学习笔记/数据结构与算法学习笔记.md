# 数据结构与算法学习笔记

**数据结构**指的是数据之间的结构组织。数据组织的结构不同，数据的存取效率就会产生巨大的差异。

数据结构的三要素分别是逻辑结构、存储结构、数据的运算。我们可以将数据结构理解为一组数据在计算机中的存储结构，或者是相互之间存在一种或多种特定关系的数据集合。

**算法**是操作数据，解决特定问题的求解步骤和方法。

算法通常具备五个特性：

1. 输入：传递给算法的参数或数据。算法可以有零个或多个输入。
2. 输出：算法处理的结果。算法必须有输出，否则算法就没有存在的意义。
3. 有穷性：在有限的步骤内执行完。不会出现无限循环，每个步骤也能在可接受的时间内完成。
4. 确定性：相同的输入只能产生唯一的输出结果。换句话说，算法的每个步骤都具有确定的含义，不会出现二义性。
5. 可行性：可以用已有的基本操作来实现算法，算法的每一步都能够通过执行有限次数来完成。

一个好的算法在设计上通常需要满足四个要求：

1. 正确性：算法能够正确反映问题的需求，能够正确解决问题。
2. 可读性：对算法的描述及实现代码要具备良好的可读性，以便于阅读、理解和交流。
3. 健壮性：输入数据不合法时，算法能做适当处理，而不是产生异常或无法预知的结果。
4. 满足高时间效率和低存储量需求：高时间效率指算法运行的速度快，节省时间，低存储量指算法运行时所需的内存空间少。



## 1.算法时间复杂度

算法的时间复杂度用于度量算法的执行时间。

### 大O时间复杂度表示法

Big O notation

$T(n)=O(f(n))$

* n，表示问题规模的大小
* T(n)，表示算法的执行时间，也就是算法的时间复杂度
* f(n)，表示代码的执行次数总和
* O，表示代码的执行时间T(n)与f(n)的函数关系(正比关系)

算法时间复杂度表示的不是代码真正的执行时间，而是代码执行时间随问题规模增长的变化趋势，或者说代码的执行时间与问题规模之间的增长关系。所以，我们也把它叫作算法的渐进时间复杂度，简称时间复杂度。

在大O时间复杂度表示法中，当算法的问题规模 n 足够大的时候，f(n)中的系数、常量、低阶都变得无关紧要，可以忽略掉，不会影响代码执行时间随问题规模增长的变化趋势。

### 算法时间复杂度计算规则

* 规则 1：只关注循环中的代码段

  在分析一个算法的时间复杂度时，我们只需要关注循环中的代码段，该代码段的执行次数对应的问题规模 n 的阶数（数量级）就是整个算法的时间复杂度。

* 规则 2：加法规则

  加法规则的算法时间复杂度取决于阶数最高的代码段的复杂度。

  $若有T1(n)=O(f(n))，T2(n)=O(g(n))，则T(n)=T1(n)+T2(n)=O(f(n))+O(g(n))=O(max(f(n),g(n)))$

* 规则 3：乘法规则

  乘法规则的算法时间复杂度取决于内外循环代码段时间复杂度的乘积。

  $若有T1(n)=O(f(n))，T2(n)=O(g(n))，则T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))$

### 常见算法时间复杂度分析

* O(1): Constant Complexity，常数阶复杂度

* O($logn$): Logarithmic Complexity，对数阶时间复杂度    

  > 讨论对数阶时间复杂度时，统一认为以2为底，而且书写时间复杂度时，通常这个底数2也忽略不写。
  >
  > 根据对数换底公式 $log_bn =log_ba* log_an$可知，对数之间可以相互转换，所以$ log_3n=log_32 log_2n$，因此 $O(log_3n)=O(log_32log_2n)$，而其中的 $log_32$是一个常数，作为系数可以忽略。

* O(n): Linear Complexity，线性阶时间复杂度

* O($nlogn$): 线性对数阶时间复杂度

* O($n^2$): N square Complexity，平方阶时间复杂度

* O($n^3$): N square Complexity，立方阶（由平方阶、立方阶，其实可以扩展出k次方阶，用O($n^k$)表示

* O($2^n$): Exponential Growth，指数阶

* O($n!$): Factorial，阶乘阶



O($2^n$)、O($n!$)、O($n^n$)这三个时间复杂度对应的算法，在问题规模n逐渐增大时，所需要的执行时间将急剧增加，效率很低，计算机往往无法承受，因此这三个时间复杂度又被称为**非多项式量级**的时间复杂度，一般很少讨论。而其余的时间复杂度被称为**多项式量级**的时间复杂度。不论怎样，在计算机可以承受的前提下，写出阶数更低的算法是每个开发者都应该追求的目标。

### 最好、最坏、平均情况时间复杂度

**最好情况时间复杂度**：代码在最理想情况下执行的时间复杂度。

**最坏情况时间复杂度**：代码在最差情况下执行的时间复杂度。

**平均情况时间复杂度**：表示平均情况下的时间复杂度。用代码在所有情况下执行的次数的加权平均值表示。

当讨论算法复杂度时，通常指最坏情况时间复杂度，平均情况时间复杂度也应该给予关注。

**均摊时间复杂度**：在代码执行的所有复杂度情况中绝大部分是低级别的复杂度，个别情况是高级别复杂度且发生具有时序关系时，可以将个别高级别复杂度均摊到低级别复杂度上。基本上均摊结果就等于低级别复杂度。

## 2.算法空间复杂度

算法的空间复杂度就是用于衡量算法运行所需要的存储空间与问题规模之间的增长关系。

算法的空间复杂度同样采用大 O 表示法，与时间复杂度很类似，引入大 O 空间复杂度表示法，有公式 $S(n) = O(f(n))$。

* n：表示问题规模的大小。
* S(n)：表示算法执行所需要的空间（S 指的是 Space），也就是算法的空间复杂度。既然 S(n) 这对圆括号中包含了一个 n，则表示算法执行所需要的空间必然与问题规模有紧密的关系。
* f(n)：表示代码的执行次数总和。f(n) 中的系数、常量、低阶同样都可以忽略掉，这一点与算法的时间复杂度一样。
* O：表示代码执行所需的空间 S(n) 与 f(n) 的函数关系（正比关系）。

### 常见算法空间复杂度分析

* O(1)：常数阶空间复杂度  

  **这种算法在执行过程中只需要固定大小内存空间的情形，也叫作算法原地工作。**

* O(n)：线性阶空间复杂度

  如果每次递归调用所需要的内存空间大小固定不变，那么算法的空间复杂度一般都是等于递归调用深度。

* $O(n^2)$：平方阶空间复杂度

## 3.数据结构

### 通用数据结构的时间空间复杂度

来源：https://www.bigocheatsheet.com/

![](./img/通用数据结构的时间空间复杂度.png)

### 数组

数组(array)是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。

数组支持随机访问，根据下标随机访问的时间复杂度为O(1)。插入、删除操作的平均情况时间复杂度为O(n)。

> 插入：如果数组中存储的数据没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第k个位置，为了避免大规模的数据搬移，有一个简单的办法就是，直接将第k位的数据搬移到数组元素的最后，把新的元素直接放入第k个位置。利用这种处理技巧，在特定场景下，在第k个位置插入一个元素的时间复杂度就会降为O(1)。这个处理思想在快排中会用到。
>
> 删除：多次删除操作集中在一起执行，提高删除效率。可以先记录下已经删除的数据，每次的删除操作并不是真正地删除数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。这正是JVM标记清除垃圾回收算法的核心思想。

数组在定义的时候需要预先指定大小，因为需要分配连续的内存空间。

针对数组类型，很多语言都提供了容器类，比如Java中的ArrayList、C++ STL中的vector。容器类与数组相比，优势有：

* 可以将很多数组操作的细节封装起来。比如数组插入、删除数据时需要搬移其他数据等。
* 支持动态扩容。

> 为什么大多数编程语言中，数组要从0开始编号，而不是从1开始呢？
>
> 从数组存储的内存模型上看，“下标”最确切的定义应该时“偏移”。如果用a来表示数组的首地址，a[0]就是偏移为0的位置，也就是首地址，a[k]就表示偏移 k 个type_size的位置，所以计算a[k]的内存地址只需要用这个公式：
>
> ```c++
> a[k]_address = base_address + k * type_size
> ```
>
> 但是，如果数组从1开始计数，那我们计算数组元素a[k]的内存地址就会变为：
>
> ```c++
> a[k]_address = base_address + (k-1) * type_size
> ```
>
> 对比两个公式，从1开始编号，每次随机访问数组元素都多了一次减法运算，对于CPU来说，就是多了一次减法指令。数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从0开始编号，而不是从1开始。

### 链表

链表通过“指针”将一组零散的内存块串联在一起。其中，我们把内存块称为链表的“结点”。

在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的，只需要考虑相邻结点的指针改变，对应的时间复杂度是O(1)。

链表要随机访问第k个元素，需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。链表随机访问的时间复杂度是O(n)。

* 单链表：只有一个方向，结点只有一个后继指针next指向后面的结点。
* 循环链表：是一种特殊的单链表，尾结点指针指向链表的头结点。
* 双向链表：支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。

双向链表可以支持O(1)时间复杂度的情况下找到前驱结点，正是这样的特点，使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。

**数组和链表的对比：**

数组简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对CPU缓存不友好，没办法有效预读。

数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容。

如果你的代码对内存的使用非常苛刻，那数组更适合。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片。如果是Java语言，就有可能会导致频繁的GC(Garbage Collection，垃圾回收)。

> 如何基于链表实现LRU缓存淘汰算法？
>
> 我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。
>
> 1.如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。
>
> 2.如果此数据没有在缓存链表中，又可以分为两种情况：
>
> * 如果此时缓存未满，则将此结点直接插入到链表的头部；
> * 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。
>
> 这种基于链表的实现思路，缓存访问的时间复杂度为O(n)。优化：引入散列表来记录每个数据的位置，将缓存访问的时间复杂度降到O(1)。

### 跳表

链表加多级索引的结构，就是跳表。

跳表(Skip list)使用空间换时间的设计思路，通过构建多级索引来提高查询的效率，实现了基于链表的“二分查找”。跳表是一种动态数据结构，支持快速地插入、删除、查找操作，时间复杂度都是O(logn)。跳表的空间复杂度是O(n)。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。很多时候，为了代码的简单、易读，比起红黑树，我们更倾向用跳表。

> 在一个具有多级索引的跳表中，查询某个数值的时间复杂度是多少呢？
>
> 如果链表里有n个结点，按照每两个结点抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是n/2，第二级索引的结点个数大约就是n/4，第三级索引的结点个数大约就是n/8，依次类推，第k级索引的结点个数是第k-1级索引的结点个数的1/2，那第k级索引的结点个数就是$n/(2^k)$。假设索引有h级，最高级的索引有2个结点。通过上面公式得到n/(2^h^)=2，从而求得$h=log_2n-1$。如果包含原始链表这一层，整个跳表的高度就是logn。我们在跳表中查询某个数据的时候，如果每一层都要遍历m个结点，那在跳表中查询一个数据的时间复杂度就是O(m*logn)。按照前面这种每两个结点抽出一个结点的索引结构，我们每一级索引都最多只需要遍历3个结点，也就是说m=3。所以在跳表中查询任意数据的时间复杂度就是O(logn)。
>
> 跳表需要存储多级索引，假设原始链表大小为n，那第一级索引大约有n/2个结点，第二级索引大约有n/4个结点，依此类推，每上升一级就减少一半，直到剩下2个结点。这几级索引的结点总和就是n/2+n/4+n/8...+8+4+2=n-2。所以，跳表的空间复杂度是O(n)。也就是说，如果将包含n个结点的单链表构造成跳表，我们需要额外再用接近n个结点的存储空间。
>
> 在实际软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。

**跳表索引动态更新**

当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某2个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡。也就是说，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。

跳表是通过随机函数来维护“平衡性”。当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值K，那我们就将这个结点添加到第一级到第K级这K级索引中。

**Redi用跳表实现有序集合**

Redis 中的有序集合是通过跳表来实现的，严格点讲，其实还用到了散列表。

Redis 中的有序集合支持的核心操作主要有：

- 插入一个数据；
- 删除一个数据；
- 查找一个数据；
- 按照区间查找数据；
- 迭代输出有序序列。

其中，插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表是一样的。但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。对于按照区间查找数据这个操作，跳表可以做到 O(logn) 的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做非常高效。

跳表相对红黑树而言代码更容易实现，简单就意味着可读性好，不容易出错。还有，跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。

**跳表的简易代码实现**：

[跳表的java实现](./code/skiplist/SkipList.java)

[跳表的python实现](./code/skiplist/SkipList.py)

### 栈

**栈(stack)**是一种特殊的线性表，其插入（也称入栈或压栈）和删除（也称出栈或弹栈）操作都在表的同一端进行。这一端称为**栈顶**(top)，另一端称为**栈底**(bottom)。栈是一个后进先出（last-in-first-out，LIFO）的数据结构。

栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，叫作**顺序栈**，用链表实现的栈，叫作**链式栈**。

入栈、出栈只涉及栈顶个别数据的操作，时间复杂度是O(1)。入栈和出栈过程中只需要一两个临时变量存储空间，空间复杂度是O(1)。

> 分析一下支持动态扩容的顺序栈的入栈、出栈操作的时间复杂度：
>
> 出栈操作不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是O(1)。但入栈操作情况不一样：当栈中有空闲空间时，入栈操作的时间复杂度为O(1)，但当空间不够时，就需要重新申请内存和数据搬移，所以时间富足度就变成了O(n)。也就是说，对于入栈操作来说，最好情况时间复杂度是O(1)，最坏情况时间复杂度是O(n)。平均情况下的时间复杂度用摊还分析法分析：如果当前栈大小为K，并且已满，当再有新的数据要入栈时，就需要重新申请2倍大小的内存，并且做K 个数据的搬移操作，然后再入栈。但是，接下来的K-1次入栈操作，都不需要再重新申请内存和搬移数据，所以这K-1次入栈操作都只需要一个simple-push操作。可以看出来，这K次入栈操作，总共涉及了K个数据的搬移，以及K次simple-push操作。将K个数据搬移均摊到K次入栈操作，那每个入栈操作只需要1个数据搬移和一个simple-push操作。依次类推，入栈操作的均摊时间复杂度就为O(1)。
>
> 在大部分情况下，入栈操作的时间复杂度都是O(1)，只有在个别时刻才会退化为O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近O(1)。

函数调用栈：

操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构，用来存储函数调用时的临时变量。每进入一个函数，就会把临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。

编译器如何利用栈来实现表达式求值：

通过两个栈来实现。其中一个保存操作数的栈，另一个保存运算符的栈。从左到右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较：如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取2个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。

栈在括号匹配中的应用：

用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。

### 队列

**队列(queue)**是一个特殊的线性表，其插入和删除操作分别在表的不同端进行。插入元素的那一端称为**队尾**(back 或 rear)，删除元素的那一端称为**队首**(front)。队列是一个先进先出（FIFO）的数据结构。

队列既可以用数组来实现，也可以用链表来实现。用数组实现的队列，叫作**顺序队列**，用链表实现的队列，叫作**链式队列**。

**循环队列**：首尾相连的队列。循环队列队空的判断条件：head == tail，队满的判断条件：(tail + 1)%n = head

**阻塞队列**：在队的基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞，因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。

基于阻塞队列实现的“生产者-消费者模型”，可以有效地协调生产和消费的速度，当“生产者”生产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续“生产”。

如何实现一个线程安全的队列？

线程安全的队列叫作**并发队列**。最简单直接的实现方式是直接在enqueue()、dequeue()方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用CAS原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。



### 哈希表

**散列表**（**Hash table**，也叫**哈希表**），是根据键（Key）而直接访问在内存存储位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录，这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做**散列表**。

### 树

一棵树 t 是一个非空的有限元素的集合，其中一个元素为根(root)，其余的元素（如果有的话）组成 t 的子树。

![](./img/树.png)

**级(level)**：树根是1级，其孩子（如果有）是2级，孩子的孩子是3级。

一棵树的**高度(height)**或**深度(depth)**是树中级的个数。

一个**元素的度(degree of an element)**是指其孩子的个数。**一棵树的度(degree of a tree)**是其元素的度的最大值。

#### 二叉树

一棵二叉树 t 是有限个元素的集合。当二叉树非空时，其中有一个元素称为根，余下的元素被划分为两棵二叉树，分别称为 t 的左子树和右子树。

```c++
 struct TreeNode {
     int val;
     TreeNode *left;
     TreeNode *right;
     TreeNode() : val(0), left(nullptr), right(nullptr) {}
     TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}
     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}
 };
```

##### 满二叉树

满二叉树的特点：

1. 所有的分支节点都存在左子树和右子树（非叶节点的度一定是 2）。

2. 所有的叶子都在同一层上（这也意味着叶节点只能出现在最下一层）。
3. 不存在度为非 0 和非 2 的节点。

满二叉树的定义：满二叉树是指一棵高度为 h，且含有 $2^h-1$ 个节点的二叉树。

##### 完全二叉树

完全二叉树的特点：

1. 叶节点都在最底下两层。
2. 最后一层的叶节点都靠左侧排列（左侧连续），并且除最后一层，其他层的节点个数都要达到最大。
3. 倒数第二层如果有叶节点，则叶节点都靠右侧排列（右侧连续）。
4. 如果节点度为 1，则该节点只有左子树，不可以只有右子树。而且最多只有一个度为 1 的节点。

**满二叉树一定是一棵完全二叉树，但完全二叉树不一定是满二叉树。**

完全二叉树的定义：一棵高度为 h 的完全二叉树，当且仅当其每个节点都与高度为 h 的满二叉树中编号为 1~n 的节点一一对应时，称为完全二叉树。

##### 二叉树的性质

* 性质一：在二叉树的第 i 层上，最多有$2^{i−1}$个节点（i≥1）

* 性质二：高度为 k 的二叉树至多有$2^k-1$ 个节点（k≥1）

* 性质三：二叉树节点的总数量等于节点的总度数 +1

* 性质四：对任何一棵二叉树，如果其叶节点数量为n0，度为 2 的节点数量为n2，则叶节点的数量比有两棵子树的节点数量多一个，即：n0=n2+ 1。

  > 非完全二叉树，除了叶节点（度为 0），其他的节点度数要么为 1 要么为 2，如果假设度为 1 的节点数量是 n1，那么该二叉树的节点总数量 n = n0 + n1 + n2。再算一算节点的总度数，节点的总度数应该等于 2 度节点数量 *2+1 度节点数量 *1，因此，节点的总度数 = 2n2 + n1。再根据性质三，节点的总数量 = 节点的总度数 + 1，就有：节点总数量 n = 2n2 + n1+ 1。结合刚才的节点总数量式子，可以得到：n0 + n1 + n2 = 2n2 + n1+ 1。两边同时减少一个 n1 和一个 n2，不难得到：n0  =  n2+ 1，得出了性质四的结论。

* 性质五：具有 n（n>0）个节点的完全二叉树的高度为$⌈log_2(n+1)⌉$ 或者 $⌊log_2n⌋ +1$。

  > 
  >
  > 第一个式子推导：![img](./img/二叉树性质五推导一.webp)
  >
  > 第二个式子推导：
  >
  > ![img](./img/二叉树性质五推导二.webp)

扩展一下性质五：一个完全二叉树的第 k 的节点的高度为$⌈log_2(k+1)⌉$ 或者 $⌊log_2k⌋ +1$。

* 性质六：如果对一棵有 n 个节点的完全二叉树的节点按层从 1 开始编号（从上到下从左到右编号），对任意节点 i（1≤i≤n），有：

  如果 i=1，则节点 i 是二叉树的根，无父节点，如果 i>1，则其父节点编号是⌊i/2⌋。

  如果 2i>n，则节点 i 为叶子节点（无孩子节点），否则，其左孩子是节点 2i。

  如果 2i+1>n，则节点 i 无右孩子（但可能有左孩子），否则其右孩子是节点 2i+1。

##### 二叉树遍历

根据根的位置

* 前序(Pre-order)：根-左-右
* 中序(In-order)：左-根-右
* 后序(Post-order)：左-右-根

#### 二叉搜索树

二叉搜索树(Binary Search Tree)，也称二叉搜索树、有序二叉树(ordered binary tree)，排序二叉树(sorted binary tree)，是指一棵空树或者具有下列性质的二叉树：

1. 若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值；
2. 若任意节点的右子树不空，则右子树上所有结点的值均大于它的根节点的值；
3. 任意节点的左 、右子树也分别为二叉查找树。

#### 字典树

Trie树，即字典树，又称单词查找树或键树，是一种树形结构，是一种哈希树的变种。典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。

它的优点是：最大限度地减少无谓的字符串比较，查询效率比哈希表高。

Trie树的核心思想是空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。

基本性质

1. 根节点不包含字符，除根节点外每一个节点都只包含一个字符。
2. 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。
3. 每个节点的所有子节点包含的字符都不相同。

### 图

#### 图的遍历

在树(图/状态集)中寻找特定节点。

* **广度优先搜索(breadth first search，BFS)**

  从一个顶点开始，搜索所有可到达顶点的方法叫做**广度优先搜索**。这种搜索方法可使用队列实现。

  ```c++
  void BFS(Node start){
      queue<xxx> q;
      q.push(start);
      set<xxx> visited;
      visited.insert(start);
      
      while(!q.empty()){
          node = q.pop();
          visited.insert(node);
          
          process(node);
          // 1.找node后继节点 2.判断后端节点没有被访问过
          nodes = generate_related_nodes(node);
          q.push(nodes);
      }
  }
  ```

  为了防止重复访问，需要一个判重的数组或集合，记录节点是否已经被访问过。

  搜索开始，把开始节点放到队列里，并标记为已经访问过。接下来对队列进行循环，只要队列不为空，我们就把队列头元素取出。取出后首先标记为已经访问过，然后进行一系列的操作。操作完后，我们将这个节点的后继节点取出来，且判断没有被访问过，则加到队列里。然后继续循环，一直到整个队列都访问完为空。

* **深度优先搜索(depth first search，DFS)**

  从一个顶点v出发，首先将v标记为已到达的顶点，然后选择一个邻接于v的尚未到达的顶点u。如果这样的u不存在，则搜索终止。假设这样的u存在，那么从u又开始一个新的DFS。

  **递归写法**

  ```c++
  set<XXX> visited;
  void DFS(Node node){
      visited.insert(node);
      
      //process current node here
      ...
      for (next_node in node.children()){
          if (!visited.count(next_node)){
              dfs(next_node)
          }
      }
  }
  ```

  标记当前节点已经访问，并对当前节点进行操作。接下来遍历当前节点的所有后继节点，如果没有被访问过，则递归调用继续访问。递归本身给我们实现了栈的数据结构来存所有的节点。

  **非递归写法**

  ```c++
  void DFS(Node root){
      set<XXX> visited;
      
      stack<XXX> stk;
      stk.push(root);
      
      while(!stk.empty()){
          node = stk.top();
          stk.pop();
          visited.insert(node);
          
          nodes = generate_related_nodes(node);
          stk.push(nodes);
      }
  }
  ```

  使用栈，每次从栈中弹出当前栈顶元素进行处理，同时把当前元素的后继节点推入到栈中。不断循环直到栈中元素为空。

## 4.算法



### 滑动窗口

滑动窗口模板化解题，五步走策略：

1、定义需要维护的变量

2、定义窗口的首尾端 (start, end)， 然后滑动窗口

3、更新需要维护的变量, 有的变量需要一个 if 语句来维护 (比如最大最小长度)

4、如果题目的窗口长度可变: 这个时候一般涉及到窗口是否合法的问题

如果当前窗口不合法时, 用一个 while 去不断移动窗口左指针, 从而剔除非法元素直到窗口再次合法

5、返回所需要的答案



### 递归

方法或函数调用自身的方式称为递归调用，调用称为递，返回称为归。

基本上，所有的递归问题都可以用递推公式来表示。

**递归的优缺点**：

1.优点：代码的表达力很强，写起来简洁。

2.缺点：空间复杂度高，有堆栈溢出风险，存在重复计算、过多的函数调用会耗时多等问题。

**递归需要满足的三个条件**：

1.一个问题的解可以分解为几个子问题的解。子问题就是数据规模更小的问题。

2.问题与子问题，除了数据规模不同，求解的思路完全一样。

3.存在递归终止条件。

**递归常见问题**：

1.警惕堆栈溢出：

系统栈或虚拟机栈空间一般都不大，如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。

可以声明一个全局变量来控制递归的深度，从而避免堆栈溢出。

2.警惕重复计算：通过某种数据结构来保存已经求解过的值，从而避免重复计算。

**如何将递归改写为非递归代码**：

抽象出递推公式、初始值和边界条件，然后用迭代循环实现。



**递归代码模板**

```c++
void recursion(level, param1, param2, ...){
    // 递归终止条件 recursion terminator
    if(level > MAX_LEVEL){
        print_result();
        return
    }
    // 当前层业务逻辑 process logic in current level
    process_data(level, data...);
    // 递归调用下一层 drill down
    recursion(level + 1, p1, ...);
    // 如果需要，返回当前层收尾工作 reverse the current status if needed
    reverse_state(level);
}
```

写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出**递推公式**，然后再推敲**终止条件**，最后将递推公式和终止条件翻译成代码。

### 分治

递归经常会在一些高阶的算法用到，其中一种是"分治(Divide & Conquer)"。

分治：先把一个大的问题剖析成子问题，子问题再一一进行解决。

![分治](./img/分治.png)

好处是可以并行计算，因为每一个子问题是互不相关的。

Note：分治可以解决的问题，是它没有中间结果，也就是没有重复计算。如果有大量重复计算，分治可能效率并不高。那时可以有更适合的算法，比如动态规划、把中间结果保存起来下次直接使用(子问题记忆)。

分治算法代码模板：

```c++
void divide_conquer(problem, param1, param2, ...){
    // 递归终止条件 recursion terminator
    if(problem is None){
        print_result();
        return;
    }
    
    // 准备数据，拆分子问题 prepare data
    data = prepare_data(problem);
    subproblems = split_problem(problem, data);
    
    // 解决子问题 conquer subproblems
    subresult1 = divide_conquer(subproblems[0], p1, ...);
    subresult2 = divide_conquer(subproblems[1], p1, ...);
    subresult3 = divide_conquer(subproblems[2], p1, ...);
    ...
    
    // 子结果合并 process and generate the final result
    result = process_result(subresult1, subresult2, subresult3, ...);
}
```



### 回溯算法

回溯（backtrack）：一种通过探索所有可能的候选解来找出所有的解的算法。如果候选解被确认不是一个解（或者至少不是最后一个解），回溯算法会通过在上一步进行一些变化抛弃该解，即回溯并且再次尝试。



回溯算法的思考步骤如下：

1、画出递归树，找到状态变量(回溯函数的参数)

2、寻找结束条件，由于回溯算法是借助递归实现，所以也就是去寻找递归终止条件

3、确定选择列表，即需要把什么数据存储到结果里面

4、判断是否需要剪枝，去判断此时存储的数据是否之前已经被存储过

5、做出选择，递归调用该函数，进入下一层继续搜索

6、撤销选择，回到上一层的状态

回溯算法解题的一个模板：

```c++
// 1、画出递归树，找到状态变量(回溯函数的参数)
private void backtrack("原始参数") {
  
    // 2、寻找结束条件，由于回溯算法是借助递归实现，所以也就是去寻找递归终止条件
    if ("终止条件") {
        // 一些逻辑操作（可有可无，视情况而定）
       // 比如，在 N 皇后问题中，在这一步把数据加入到了结果里面
       添加操作
        return;
    }
  
   // 3、确定选择列表，即需要把什么数据存储到结果里面
   // for 循环就是一个选择的过程
    for ("遍历本层集合中元素") {
      
        // 一些逻辑操作（可有可无，视情况而定）
       // 4、判断是否需要剪枝，去判断此时存储的数据是否之前已经被存储过
    
        // 5、做出选择，递归调用该函数，进入下一层继续搜索
        // 递归
        backtrack("新的参数");
      
        // 一些逻辑操作（可有可无，视情况而定）

        // 6、撤销选择，回到上一层的状态
    }
}
```

### 二分查找

二分查找(Binary Search)是针对有序数据的高效查找算法，时间复杂度O(logn)。核心思想：每次都通过跟区间中的中间元素对比，将待查找的区间缩小为一半，直到找到要查找的元素，或者区间被缩小为0。

三个要求：

1. Sorted（查找的数在一个有序的数组里，单调递增或者递减）
2. Bounded（存在上下界）
3. Accessible by index（能够通过索引访问）

二分查找代码模板

```c++
left, right = 0, len(array) - 1
while left <= right:
	mid = left + (right - left) / 2
    if array[mid] == target:
		//find the target!!
		break or return result
    elif array[mid] < target:
		left = mid + 1
    else:
		right = mid - 1
```

把一个递增的数组一分为二，每次和中间数比较，小于中间数则在左边继续查找，大于中间数则在右边继续查找，找到则返回

需要着重掌握三个容易出错的地方：

* 循环退出条件

  注意是low <= high，而不是low < high

* mid的取值

  mid = (low + high)/2这种写法有问题，因为如果low和high比较大的话，两者之和就有可能溢出。改进方法是写成 low + (high - low)/2。更进一步，如果要将性能优化到极致的话，可以将除以2转化为位运算 low + (high - low) >> 1。

* low和high的更新

  low = mid + 1，high = mid - 1。注意这里的+1和-1，如果直接写成low=mid或high=mid，就可能发生死循环。

二分查找虽然性能比较优秀，但应用场景也比较有限。底层必须依赖数组，并且还要求数据是有序的。对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。

#### 二分查找的变形问题

* 查找第一个值等于给定值的元素

  ```c++
  public int bsearch(int[] a, int n, int value){
      int low = 0;
      int high = n - 1;
      while(low <= high){
          int mid = low + (high - low) >> 1;
          if(a[mid] > value){
              high = mid - 1;
          }else if(a[mid] < value){
              low = mid + 1;
          }else{
              if((mid == 0) || a[mid - 1] != value)) return mid;
              else high = mid - 1;
          }
      }
      return -1;
  }
  ```

  如果mid等于0，那这个元素已经是数组的第一个元素，那它肯定是我们要找的；如果mid不等于0，但 a[mid]的前一个元素 a[mid-1]不等于value，那也说明 a[mid] 就是我们要找的第一个值等于给定值的元素。如果经过检查之后发现 a[mid]前面的一个元素 a[mid-1]也等于value，那说明此时的 a[mid]肯定不是我们要查找的第一个值等于给定值的元素。那我们就更新high=mid -1，因为要找的元素肯定出现在[low, mid-1]之间。

* 查找最后一个值等于给定值的元素

  ```c++
  public int bsearch(int[] a, int n, int value){
      int low = 0;
      int high = n - 1;
      while(low <= high){
          int mid = low + (high - low) >> 1;
          if(a[mid] > value){
              high = mid - 1;
          }else if(a[mid] < value){
              low = mid + 1;
          }else{
              if((mid == n - 1) || a[mid + 1] != value)) return mid;
              else low = mid + 1;
          }
      }
      return -1;
  }
  ```

  如果a[mid]这个元素已经是数组中的最后一个元素了，那它肯定是我们要找的；如果 a[mid]的后一个元素 a[mid+1]不等于value，那也说明 a[mid] 就是我们要找的最后一个值等于给定值的元素。如果经过检查之后发现 a[mid]后面的一个元素 a[mid+1]也等于value，那说明此时的 a[mid]并不是我们要查找的最后一个值等于给定值的元素。那我们就更新low=mid +1，因为要找的元素肯定出现在[mid+1, high]之间。

* 查找第一个大于等于给定值的元素

  ```c++
  public int bsearch(int[] a, int n, int value){
      int low = 0;
      int high = n - 1;
      while(low <= high){
          int mid = low + (high - low) >> 1;
          if(a[mid] >= value){
              if((mid == 0) || (a[mid - 1] < value)) return mid;
              else high = mid - 1;
          }else{
              low = mid + 1;
          }
      }
      return -1;
  }
  ```

* 查找最后一个小于等于给定值的元素

  ```c++
  public int bsearch(int[] a, int n, int value){
      int low = 0;
      int high = n - 1;
      while(low <= high){
          int mid = low + (high - low) >> 1;
          if(a[mid] > value){
  			high = mid - 1;
          }else{
              if((mid == n - 1) || (a[mid + 1] > value)) return mid;
              else low = mid + 1;
          }
      }
      return -1;
  }
  ```

凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或者二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多。实际上，求"值等于给定值"的二分查找确实不怎么会被用到，二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。比如上面这几种变体问题，用其他数据结构，比如散列表，二叉树，就比较难实现了。

### 贪心算法

贪心法(Greedy)，又称贪心算法、贪婪算法：在对问题求解时，总是做出在当前看来时最好的选择。

适用Greedy的场景：

简单地说，问题能够分解成子问题来解决，子问题的最优解能递推到最终问题的最优解。这种子问题最优解称为最优子结构。

贪心算法与动态规划的不同在于它对每个子问题的解决方案都做出选择，不能回退。动态规划则会保存以前的运算结果，并根据以前的结果对当前进行选择，有回退功能。

从人的思想来看：贪心算法每次只做当前最优的选择，目光短线。而动态规划每次做选择时会综合考虑几种方案，而且还会保存下来，然后一步步走，可以回到之前的状态再做调整，最后调整出最优的方案。

### 动态规划

DP状态的定义

DP方程
